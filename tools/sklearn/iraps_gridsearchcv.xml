<tool id="iraps_gridsearchcv" name="IRAPS GridSearch CV" version="@VERSION@">
    <description>for hyperparameter optimization using cross validation</description>
    <macros>
        <import>main_macros.xml</import>
    </macros>
    <expand macro="python_requirements"/>
    <expand macro="macro_stdio"/>
    <version_command>echo "@VERSION@"</version_command>
    <command>
        <![CDATA[
        #if $save:
        python '$__tool_directory__/iraps_gridsearchcv.py'
            '$inputs'
            '$input_options.infile1'
            '$input_options.infile2'
            '$outfile_result'
            '$outfile_estimator'
        #else:
        python '$__tool_directory__/iraps_gridsearchcv.py'
            '$inputs'
            '$input_options.infile1'
            '$input_options.infile2'
            '$outfile_result'
        #end if
        ]]>
    </command>
    <configfiles>
        <inputs name="inputs" />
    </configfiles>
    <inputs>
        <conditional name="search_schemes">
            <param name="selected_search_scheme" type="select" label="Select a model selection search scheme:">
                <option value="GridSearchCV" selected="true">GridSearchCV - Exhaustive search over specified parameter values for an estimator </option>
                <!--<option value="RandomizedSearchCV">RandomizedSearchCV - Randomized search on hyper parameters for an estimator</option> -->
            </param>
            <when value="GridSearchCV">
                <section name="search_params_builder" title="Search parameters Builder" expanded="true">
                    <repeat name="param_set" min="1" max="5" title="Parameter setting for search:">
                        <conditional name="search_param_selector">
                        <param name="selected_param_type" type="select" label="Choose the parameter type">
                            <option value="p_thres" selected="true">P_value threshold</option>
                            <option value="fc_thres">Fold change threshold</option>
                            <option value="occurance">Occurance</option>
                            <option value="n_iter">Iterative sampling times</option>
                            <option value="clf_thres">Correlation coefficient cutoff</option>
                        </param>
                        <when value="p_thres">
                            <expand macro="search_param_input" label="Search for p_value threshold" help="For example: [0.00001, 0.0001, 0.001]."/>
                        </when>
                        <when value="fc_thres">
                            <expand macro="search_param_input" label="Search for fold change threshold" help="For example: with_centering: [0.01, 0.05, 0.1]."/>
                        </when>
                        <when value="occurance">
                            <expand macro="search_param_input" label="Search for occurance" help="For example: [0.7, 0.75, 0.8]. "/>
                        </when>
                        <when value="n_iter">
                            <expand macro="search_param_input" label="Search for iterative sampling times" help="For example: [1000]. Default 1000"/>
                        </when>
                        <when value="clf_thres">
                            <expand macro="search_param_input" label="Search for correlation coefficient cutoff" help="For example: [0.4, 0.5]."/>
                        </when>
                        </conditional>
                    </repeat>
                </section>
                <section name="options" title="Advanced Options for SearchCV" expanded="false">
                    <expand macro="search_cv_options"/>
                </section>
            </when>
            <!--<when value="RandomizedSearchCV">
                <expand macro="search_cv_estimator"/>
                <section name="options" title="Advanced Options for SearchCV" expanded="false">
                    <expand macro="search_cv_options"/>
                    <param argument="n_iter" type="integer" value="10" label="Number of parameter settings that are sampled"/>
                    <expand macro="random_state"/>
                </section>
            </when> -->
        </conditional>
        <param name="save" type="boolean" truevalue="booltrue" falsevalue="boolflase" checked="true" label="Save the best estimator/pipeline?"/>
        <expand macro="sl_mixed_input"/>
    </inputs>
    <outputs>
        <data format="tabular" name="outfile_result"/>
        <data format="zip" name="outfile_estimator">
            <filter>save</filter>
        </data>
    </outputs>
    <tests>
        <test>
            <param name="selected_search_scheme" value="GridSearchCV"/>
            <param name="infile_pipeline" value="pipeline01" ftype="zip"/>
            <conditional name="search_param_selector">
                <param name="search_p" value="C: [1, 10, 100, 1000]"/>
                <param name="selected_param_type" value="final_estimator_p"/>
            </conditional>
            <conditional name="search_param_selector">
                <param name="search_p" value="k: [-1, 3, 5, 7, 9]"/>
                <param name="selected_param_type" value="prep_2_p"/>
            </conditional>
            <param name="error_score" value="false"/>
            <param name="infile1" value="regression_X.tabular" ftype="tabular"/>
            <param name="header1" value="true" />
            <param name="selected_column_selector_option" value="all_columns"/>
            <param name="infile2" value="regression_y.tabular" ftype="tabular"/>
            <param name="header2" value="true" />
            <param name="selected_column_selector_option2" value="all_columns"/>
            <output name="outfile_result">
                <assert_contents>
                    <has_n_columns n="13"/>
                    <has_text text="0.7938837807353147"/>
                    <has_text text="{'estimator__C': 1, 'preprocessing_2__k': 9}"/>
                </assert_contents>
            </output>
        </test>
    </tests>
    <help>
        <![CDATA[
**What it does**
Searches optimized parameter values for an estimator or pipeline through either exhaustive grid cross validation search or Randomized cross validation search.
please refer to `Scikit-learn model_selection GridSearchCV`_, `Scikit-learn model_selection RandomizedSearchCV`_ and `Tuning hyper-parameters`_.

**How to choose search patameters?**

Please refer to `svm`_, `linear_model`_, `ensemble`_, `naive_bayes`_, `tree`_, `neighbors`_ and `xgboost`_ for estimator parameters.
Refer to `sklearn.preprocessing`_, `feature_selection`_, `decomposition`_, `kernel_approximation`_, `cluster.FeatureAgglomeration`_ and `skrebate`_ for parameter in the pre-processing steps.

**Search parameter input** accepts parameter and setting in key:value pair. One pair per input box. Setting can be list, numpy array, or distribution.
The evaluation of settings supports operations in Math, list comprehension, numpy.arange(np_arange), most numpy.random(e.g., np_random_uniform) and some scipy.stats(e.g., scipy_stats_zipf) classes or functions, and others.

**Examples:**

- K: [3, 5, 7, 9]

- n_estimators: list(range(50, 1001, 50))

- gamma: np_arange(0.01, 1, 0.1)

- alpha: np_random_choice(list(range(1, 51)) + [None], size=20)

- max_depth: scipy_stats_randin(1, 11)

**Estimator search/eval (additional '-')**::

     base_estimator-: [sklearn_tree.DecisionTreeRegressor(), sklearn_tree.ExtraTreeRegressor()]

**Preprocessors search/swap**::

     : [sklearn_feature_selection.SelectKBest(), sklearn_feature_selection.VarianceThreshold(),
        skrebate_ReliefF(), sklearn_preprocessing.RobustScaler()]

**Hot number/keyword for preprocessors**::

    0   sklearn_preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)
    1   sklearn_preprocessing.Binarizer(copy=True, threshold=0.0)
    2   sklearn_preprocessing.Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)
    3   sklearn_preprocessing.MaxAbsScaler(copy=True)
    4   sklearn_preprocessing.Normalizer(copy=True, norm='l2')
    5   sklearn_preprocessing.MinMaxScaler(copy=True, feature_range=(0, 1))
    6   sklearn_preprocessing.PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)
    7   sklearn_preprocessing.RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True, with_scaling=True)
    8   sklearn_feature_selection.SelectKBest(k=10, score_func=<function f_classif at 0x113806d90>)
    9   sklearn_feature_selection.GenericUnivariateSelect(mode='percentile', param=1e-05, score_func=<function f_classif at 0x113806d90>)
    10  sklearn_feature_selection.SelectPercentile(percentile=10, score_func=<function f_classif at 0x113806d90>)
    11  sklearn_feature_selection.SelectFpr(alpha=0.05, score_func=<function f_classif at 0x113806d90>)
    12  sklearn_feature_selection.SelectFdr(alpha=0.05, score_func=<function f_classif at 0x113806d90>)
    13  sklearn_feature_selection.SelectFwe(alpha=0.05, score_func=<function f_classif at 0x113806d90>)
    14  sklearn_feature_selection.VarianceThreshold(threshold=0.0)
    15  sklearn_decomposition.FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None,
        noise_variance_init=None, random_state=0, svd_method='randomized', tol=0.01)
    16  sklearn_decomposition.FastICA(algorithm='parallel', fun='logcosh', fun_args=None,
        max_iter=200, n_components=None, random_state=0, tol=0.0001, w_init=None, whiten=True)
    17  sklearn_decomposition.IncrementalPCA(batch_size=None, copy=True, n_components=None, whiten=False)
    18  sklearn_decomposition.KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto',
        fit_inverse_transform=False, gamma=None, kernel='linear', kernel_params=None, max_iter=None,
        n_components=None, random_state=0, remove_zero_eig=False, tol=0)
    19  sklearn_decomposition.LatentDirichletAllocation(batch_size=128, doc_topic_prior=None, evaluate_every=-1, learning_decay=0.7,
        learning_method=None, learning_offset=10.0, max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001, n_components=10,
        n_topics=None, perp_tol=0.1, random_state=0, topic_word_prior=None, total_samples=1000000.0, verbose=0)
    20  sklearn_decomposition.MiniBatchDictionaryLearning(alpha=1, batch_size=3, dict_init=None, fit_algorithm='lars',
        n_components=None, n_iter=1000, random_state=0, shuffle=True, split_sign=False, transform_algorithm='omp',
        transform_alpha=None, transform_n_nonzero_coefs=None, verbose=False)
    21  sklearn_decomposition.MiniBatchSparsePCA(alpha=1, batch_size=3, callback=None, method='lars', n_components=None,
        n_iter=100, random_state=0, ridge_alpha=0.01, shuffle=True, verbose=False)
    22  sklearn_decomposition.NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,
        n_components=None, random_state=0, shuffle=False, solver='cd', tol=0.0001, verbose=0)
    23  sklearn_decomposition.PCA(copy=True, iterated_power='auto', n_components=None, random_state=0, svd_solver='auto', tol=0.0, whiten=False)
    24  sklearn_decomposition.SparsePCA(U_init=None, V_init=None, alpha=1, max_iter=1000, method='lars',
        n_components=None, random_state=0, ridge_alpha=0.01, tol=1e-08, verbose=False)
    25  sklearn_decomposition.TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5, random_state=0, tol=0.0)
    26  sklearn_kernel_approximation.Nystroem(coef0=None, degree=None, gamma=None, kernel='rbf',
        kernel_params=None, n_components=100, random_state=0)
    27  sklearn_kernel_approximation.RBFSampler(gamma=1.0, n_components=100, random_state=0)
    28  sklearn_kernel_approximation.AdditiveChi2Sampler(sample_interval=None, sample_steps=2)
    29  sklearn_kernel_approximation.SkewedChi2Sampler(n_components=100, random_state=0, skewedness=1.0)
    30  sklearn_cluster.FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto', connectivity=None,
        linkage='ward', memory=None, n_clusters=2, pooling_func=<function mean at 0x113078ae8>)
    31  skrebate_ReliefF(discrete_threshold=10, n_features_to_select=10, n_neighbors=100, verbose=False)
    32  skrebate_SURF(discrete_threshold=10, n_features_to_select=10, verbose=False)
    33  skrebate_SURFstar(discrete_threshold=10, n_features_to_select=10, verbose=False)
    34  skrebate_MultiSURF(discrete_threshold=10, n_features_to_select=10, verbose=False)
    35  skrebate_MultiSURFstar(discrete_threshold=10, n_features_to_select=10, verbose=False)
    'sk_prep_all':   All sklearn preprocessing estimators, i.e., 0-7
    'fs_all':        All feature_selection estimators, i.e., 8-14
    'decomp_all':    All decomposition estimators, i.e., 15-25
    'k_appr_all':    All kernel_approximation estimators, i.e., 26-29
    'reb_all':       All skrebate estimators, i.e., 31-35
    'all_0':         All except the imbalanced-learn samplers, i.e., 0-35
    'imb_all':       All imbalanced-learn sampling methods, i.e., 36-54.
                     **CAUTION**: Mix of imblearn and other preprocessors may not work.
     None:           opt out of preprocessor

Support mix (CAUTION: Mix of imblearn and other preprocessors may not work), e.g.::

     : [None, 'sk_prep_all', 22, 'k_appr_all', sklearn_feature_selection.SelectKBest(k=50)]


.. _`Scikit-learn model_selection GridSearchCV`: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
.. _`Scikit-learn model_selection RandomizedSearchCV`: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html
.. _`Tuning hyper-parameters`: http://scikit-learn.org/stable/modules/grid_search.html

.. _`svm`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm
.. _`linear_model`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model
.. _`ensemble`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble
.. _`naive_bayes`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes
.. _`tree`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree
.. _`neighbors`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors
.. _`xgboost`: https://xgboost.readthedocs.io/en/latest/python/python_api.html

.. _`sklearn.preprocessing`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing
.. _`feature_selection`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection
.. _`decomposition`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition
.. _`kernel_approximation`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.kernel_approximation
.. _`cluster.FeatureAgglomeration`: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html
.. _`skrebate`: https://epistasislab.github.io/scikit-rebate/using/

        ]]>
    </help>
    <expand macro="sklearn_citation">
        <expand macro="skrebate_citation"/>
        <expand macro="xgboost_citation"/>
        <expand macro="imblearn_citation"/>
    </expand>
</tool>
